{"componentChunkName":"component---src-templates-thought-tsx","path":"/2021-03-25-let's-play-with-emotions/","result":{"data":{"thought":{"slug":"/2021-03-25-let's-play-with-emotions/","title":"Let’s play with emotions!","info":"Fine-tuning BERT tutorial with PyTorch for emotion classification.","date":"March 25, 2021","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Let’s play with emotions!\",\n  \"date\": \"2021-03-25T00:00:00.000Z\",\n  \"info\": \"Fine-tuning BERT tutorial with PyTorch for emotion classification.\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"lets-play-with-emotions\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h1\"\n  }, {\n    \"href\": \"#lets-play-with-emotions\",\n    \"aria-label\": \"lets play with emotions permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Let\\u2019s Play with Emotions!\"), mdx(\"p\", null, \"In today\\u2019s tutorial, we will play with emotions, quite literally! With the growing trends in machine learning and importance to emulate human character/ traits by machines, it is vital that we understand the importance of emotion in our day to day life. In today\\u2019s world, emotional context can be applied to every aspect of our life, be it a morning meme you scroll through to , or the serenity in music, or a kid describing his first day at the guitar class! \"), mdx(\"h5\", {\n    \"id\": \"the-point-i-would-like-to-make-here-is---information-when-analysed-for-emotion-can-often-reveal-surprising-traits\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h5\"\n  }, {\n    \"href\": \"#the-point-i-would-like-to-make-here-is---information-when-analysed-for-emotion-can-often-reveal-surprising-traits\",\n    \"aria-label\": \"the point i would like to make here is   information when analysed for emotion can often reveal surprising traits permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"em\", {\n    parentName: \"h5\"\n  }, \"\\u201CThe point I would like to make here is :  Information when analysed for emotion, can often reveal surprising traits!\\u201D\")), mdx(\"p\", null, \"Recommender systems use this underlying trait to provide best class and relevant suggestions. From Amazon to Youtube, the product recommendations to the next autoplay video, emotions are consumed in every step! Hard to believe? Go try it yourself !\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"650px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/0c9c0898372223d8185af260b3f2f930/0f5a1/emotions.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"66.87116564417178%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABAABA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHqo2Sqjyf/xAAZEAEBAAMBAAAAAAAAAAAAAAABAAIDEiH/2gAIAQEAAQUCydjGXnZOLcs6xv/EABYRAAMAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAwEBPwGBf//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAECAQE/Aaf/xAAZEAACAwEAAAAAAAAAAAAAAAAAIQIQMRH/2gAIAQEABj8CUuK9NGf/xAAdEAACAgEFAAAAAAAAAAAAAAAAASFBURExYXGh/9oACAEBAAE/IamhehNyrGh7mAngXtrJ06D/2gAMAwEAAgADAAAAEOjf/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAwEBPxATuyF//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERQf/aAAgBAgEBPxCphWf/xAAbEAEAAwEBAQEAAAAAAAAAAAABABEhMVFBgf/aAAgBAQABPxBW2sYB25riQKOsriIqwKxdeW/YtRyn07+ogOkArMJ//9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"fake news\",\n    \"title\": \"fake news\",\n    \"src\": \"/static/0c9c0898372223d8185af260b3f2f930/6aca1/emotions.jpg\",\n    \"srcSet\": [\"/static/0c9c0898372223d8185af260b3f2f930/d2f63/emotions.jpg 163w\", \"/static/0c9c0898372223d8185af260b3f2f930/c989d/emotions.jpg 325w\", \"/static/0c9c0898372223d8185af260b3f2f930/6aca1/emotions.jpg 650w\", \"/static/0c9c0898372223d8185af260b3f2f930/7c09c/emotions.jpg 975w\", \"/static/0c9c0898372223d8185af260b3f2f930/01ab0/emotions.jpg 1300w\", \"/static/0c9c0898372223d8185af260b3f2f930/0f5a1/emotions.jpg 4847w\"],\n    \"sizes\": \"(max-width: 650px) 100vw, 650px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"                        source: unsplash.com\\n\")), mdx(\"h2\", {\n    \"id\": \"problem-at-hand\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#problem-at-hand\",\n    \"aria-label\": \"problem at hand permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Problem at Hand\"), mdx(\"p\", null, \"Before we dive into emotional classification, let us understand sentiment analysis and how emotions are linked to it. Sentiment analysis is an effective method to portray sentiment polarity. Consider a movie review or a product review, sentiment analysis is used to capture public sentiment in reaction to a particular brand, which in turn influences future business decisions.  If you like a movie, or favor a product, you will talk positively about it. This inturn will drive sales or increase viewership in case of a movie. However, a definite flaw in this method is that we can categorize a text either into positive/negative class, when there are so many possible emotions attached to it. A positive or a negative review does not justify the emotions attached to it. A movie you liked or you loved will still be categorized as a positive sentiment. However, it misses the intensity of liking towards the movie.  The answer to this problem is Emotion Classification and today we are going to drill on that a bit. So with a cheerful face, let\\u2019s begin!\"), mdx(\"h2\", {\n    \"id\": \"the-emotional-data\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#the-emotional-data\",\n    \"aria-label\": \"the emotional data permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"The Emotional data\"), mdx(\"p\", null, \"Consider a sentence :\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" \", mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"\\u2018I wish I could go to play today\\u2019\"), \" \"), \". Now if someone asks you what is the emotion exhibited here, I am sure you would say :sadness:. However, on looking closely, you will notice that the sentence has no negative word that can help the model classify it as a \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" \\u201Csad\\u201D \"), \" sentence. This is the first challenge we face in emotion classification!  Adding to this, it is tricky to teach machines sarcasm,irony and indirect meanings! \"), mdx(\"p\", null, \"Today we will deal with a document dataset created by \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://huggingface.co/\"\n  }), \"the hugging team \"), \". Dataset has three files, split into training, test and validation sets. An example of the data is as below : \"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"i feel like I am still looking at a blank canvas blank pieces of paper;sadness\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"p\", null, \"The dataset has \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" 20000 \"), \" rows with \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" 6 \"), \" different possible classes namely: \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" Anger, Fear, Joy, Love, Sadness, Surprise. \")), mdx(\"h2\", {\n    \"id\": \"approach-to-emotional-classification\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#approach-to-emotional-classification\",\n    \"aria-label\": \"approach to emotional classification permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Approach to Emotional Classification\"), mdx(\"p\", null, \"We can build good sentiment classifiers even with only reasonably modest-size label training sets. Plethora of pre-trained word embeddings are readily available such as Word2Vec, GloVe, Fasttext, ConceptNet NumberBatch, etc. However, these embeddings are  non-polysemic in nature, translating to only one representation of a word despite its occurrence in different contexts. Considering this shortcoming, in this scope, we will explore embeddings from Google\\u2019s BERT model. \"), mdx(\"h2\", {\n    \"id\": \"what-is-the-bert-model\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#what-is-the-bert-model\",\n    \"aria-label\": \"what is the bert model permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"What is the BERT model?\"), mdx(\"p\", null, \"BERT stands for Bidirectional Encoder Representations from Transformers and is a NLP model developed by Google for pre-training language representations. It leverages an enormous amount of plain text data publicly available (Wikipedia and Google Books) on the web and is trained in an unsupervised manner. It is a powerful model that is trained to learn the language structure and it\\u2019s nuances by training a Language Model. BERT has a deep bi-directional structure to it unlike ELMo, which is a shallow bi-directional and OpenAI GPT which is uni-directional in nature. Bidirectional nature helps the model to capture the context from previous words and words ahead of it any given time . I would recommend \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://jalammar.github.io/illustrated-bert/\"\n  }), \"reading this\"), \" in order to understand functionality of BERT even better.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"But first, setup: \"))), mdx(\"h4\", {\n    \"id\": \"exploiting-google-colab-gpu\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h4\"\n  }, {\n    \"href\": \"#exploiting-google-colab-gpu\",\n    \"aria-label\": \"exploiting google colab gpu permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Exploiting Google Colab GPU\"), mdx(\"p\", null, \"Google Colab offers free GPUs and TPUs! BERT models consume huge memory and to aid it, it is advisable to take advantage of GPU. \"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\nnum_gpu = torch.cuda.device_count()\\ntorch.cuda.get_device_name(0)\\nSEED = 9\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\nif device == torch.device(\\\"cuda\\\"):\\n    torch.cuda.manual_seed_all(SEED) \",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" Note: \"), \"  For torch to use the GPU, we need to identify and specify the GPU as the device. This will mainly come into picture when we train the model. Ayt that time, we will load the data onto the device.\"), mdx(\"p\", null, \"Next, let\\u2019s install the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/huggingface/transformers\"\n  }), \"transformers\"), \" package from Hugging Face which will give us a pytorch interface for working with BERT.At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built  modifications of these models suited to your specific task. For example, in this tutorial we will use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BertForSequenceClassification\"), \".\"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"!pip install transformers\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"h2\", {\n    \"id\": \"preprocessing-the-data\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#preprocessing-the-data\",\n    \"aria-label\": \"preprocessing the data permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Preprocessing the Data\"), mdx(\"p\", null, \"Once we download and parse our data, it is time to encode our labels set. BEfore we encode our classes, we need to merge the three files to have the same label encoding.  \"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"df = pd.concat([df_train,df_test,df_val])\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"p\", null, \"We will use the LabelEncoder from sckit package.\"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"labelencoder = LabelEncoder()\\ndf['label_enc'] = labelencoder.fit_transform(df['label'])\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"p\", null, \"Let\\u2019s extract the sentences and labels of our training set as numpy ndarrays.\"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"sentences = df.sentance.values\\nlabels = df.label.values\\n\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"br\", null), mdx(\"br\", null), mdx(\"h2\", {\n    \"id\": \"tokenization-and-attention-masks\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#tokenization-and-attention-masks\",\n    \"aria-label\": \"tokenization and attention masks permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Tokenization and Attention Masks\"), mdx(\"p\", null, \"BERT models take in a specific format. In order to do so, we need to transform our data into tokens.\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"650px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/b2ac1d2c98f61e544f9243a6cab29bb7/9e23d/token.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"31.288343558282207%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdiEAf/EABYQAQEBAAAAAAAAAAAAAAAAAAARMf/aAAgBAQABBQLUR//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAABD/2gAIAQEABj8Cf//EABcQAAMBAAAAAAAAAAAAAAAAAAABgSH/2gAIAQEAAT8heEyCD//aAAwDAQACAAMAAAAQi+//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQMBAT8Qq6//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQIBAT8QiY//xAAbEAEAAgIDAAAAAAAAAAAAAAABAEFx0RGR8f/aAAgBAQABPxAoTV5zOfh3F09Hc//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Bert model picture\",\n    \"title\": \"Bert model picture\",\n    \"src\": \"/static/b2ac1d2c98f61e544f9243a6cab29bb7/6aca1/token.jpg\",\n    \"srcSet\": [\"/static/b2ac1d2c98f61e544f9243a6cab29bb7/d2f63/token.jpg 163w\", \"/static/b2ac1d2c98f61e544f9243a6cab29bb7/c989d/token.jpg 325w\", \"/static/b2ac1d2c98f61e544f9243a6cab29bb7/6aca1/token.jpg 650w\", \"/static/b2ac1d2c98f61e544f9243a6cab29bb7/7c09c/token.jpg 975w\", \"/static/b2ac1d2c98f61e544f9243a6cab29bb7/9e23d/token.jpg 1174w\"],\n    \"sizes\": \"(max-width: 650px) 100vw, 650px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"                      Source: BERT [Devlin et al., 2018], with modifications\\n\")), mdx(\"p\", null, \"The input text to BERT must be split into tokens, further on these tokens must be mapped to their index in the tokenizer vocabulary. Tokenization is performed using the tokenizer provided with BERT. \"), mdx(\"br\", null), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"MAX_LEN = 256\\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\\nlabels = df.label.values\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"p\", null, \"As you must have noticed, we had \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"strong\"\n  }, \"add_special_tokens=True\"), \" \")), mdx(\"p\", null, \"At the end of every sentence, we need to append the special \", \"[SEP]\", \" token and prepend the special \", \"[CLS]\", \" token to the beginning of every sentence.\"), mdx(\"p\", null, \"[SEP]\", \" is used mainly to separate two sentences and describe something among them. Eg: Is A and B related to one another .\\n\", \"[CLS]\", \" has special importance in the classification task.\\nThe final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\\u201D\"), mdx(\"p\", null, \"Next important parameter is the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"strong\"\n  }, \"max_length\")), \" .\"), mdx(\"p\", null, \"Did you notice how each input sentence has a varying length. In order to handle this variation, we can either append, or truncate to a single, fixed length. The maximum sentence length, BERT can handle is 512 tokens. In our case, we will restrict the max length to  256 tokens. For a sentence smaller than 256 tokens, the remaining space is padded with \", \"[PAD]\", \" token with index 0 in BERT vocabulary.\"), mdx(\"p\", null, \"Now that we have tokenized  and added special tokens, we need to let BERT know where to pay attention. This is done via an attention mask. The \\u201CAttention Mask\\u201D is simply an array of 1s and 0s indicating which tokens are padding and which aren\\u2019t. Further reading can be done \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://huggingface.co/transformers/\"\n  }), \"here\"), \" to get a better understanding of this.\"), mdx(\"br\", null), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"## Create attention mask\\nattention_masks = []\\n## Create a mask of 1 for all input tokens and 0 for all padding tokens\\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids] \",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"h2\", {\n    \"id\": \"train-and-validation-split\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#train-and-validation-split\",\n    \"aria-label\": \"train and validation split permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Train and Validation Split\"), mdx(\"p\", null, \"Divide up our training set to use 90% for training and 10% for validation.\"), mdx(\"br\", null), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\\ntrain_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1) \",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"h2\", {\n    \"id\": \"pytorch-data-types-and-data-loaders\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#pytorch-data-types-and-data-loaders\",\n    \"aria-label\": \"pytorch data types and data loaders permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Pytorch Data Types and Data loaders\"), mdx(\"p\", null, \"The numpy.ndarrays needs to be converted into pytorch tensors. Later on we need to wrap them in a data loader . This would help us save memory during training. We do need to iterate over the entire dataset and load in memory at once. Notice how we have two data loaders defined for training and validation.\"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"# convert all our data into torch tensors, required data type for our model\\ntrain_inputs = torch.tensor(train_inputs)\\nvalidation_inputs = torch.tensor(validation_inputs)\\ntrain_labels = torch.tensor(train_labels)\\nvalidation_labels = torch.tensor(validation_labels)\\ntrain_masks = torch.tensor(train_masks)\\nvalidation_masks = torch.tensor(validation_masks)\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"br\", null), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"batch_size = 32\\ntrain_data = TensorDataset(train_inputs,train_masks,train_labels)\\ntrain_sampler = RandomSampler(train_data)\\ntrain_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\\n## validation dataloader\\nvalidation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\\nvalidation_sampler = RandomSampler(validation_data)\\nvalidation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"h2\", {\n    \"id\": \"bert-for-sequence-classification\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#bert-for-sequence-classification\",\n    \"aria-label\": \"bert for sequence classification permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Bert For Sequence Classification\"), mdx(\"p\", null, \"Now that we have the right data format and our data loader is ready , it is time to train the model. We will use the classification model from BERT. .For classification, the normal BERT model is added with a single linear layer on top for classification. \"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"650px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/28642b36bc64f6b0966c08e63eeb711e/614df/linear.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"67.48466257668711%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdpaqUAP/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAECEBES/9oACAEBAAEFAp21pytP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhABAQEAAAAAAAAAAAAAAAAAMQAg/9oACAEBAAY/Amc//8QAGxABAAICAwAAAAAAAAAAAAAAAQARECExYXH/2gAIAQEAAT8hGzhELOsIaLjQ8x//2gAMAwEAAgADAAAAEJvP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGRABAQADAQAAAAAAAAAAAAAAAQARITFR/9oACAEBAAE/EAUeE9MQugXkiV01h2NF/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"image of bert classification model\",\n    \"title\": \"image of bert classification model\",\n    \"src\": \"/static/28642b36bc64f6b0966c08e63eeb711e/6aca1/linear.jpg\",\n    \"srcSet\": [\"/static/28642b36bc64f6b0966c08e63eeb711e/d2f63/linear.jpg 163w\", \"/static/28642b36bc64f6b0966c08e63eeb711e/c989d/linear.jpg 325w\", \"/static/28642b36bc64f6b0966c08e63eeb711e/6aca1/linear.jpg 650w\", \"/static/28642b36bc64f6b0966c08e63eeb711e/7c09c/linear.jpg 975w\", \"/static/28642b36bc64f6b0966c08e63eeb711e/614df/linear.jpg 978w\"],\n    \"sizes\": \"(max-width: 650px) 100vw, 650px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"                       source: jalammar.github.io/illustrated-bert/\\n\")), mdx(\"p\", null, \"We load the BERT \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"strong\"\n  }, \"bert-base-uncased\")), \" . uncased refers to lowercase while base refers to the smaller version model .\\nIn transformers, optimizer and schedules are instantiated like this\"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"model = BertForSequenceClassification.from_pretrained(\\\"bert-base-uncased\\\", num_labels=6).to(device) \",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"h3\", {\n    \"id\": \"setting-up-hyperparamters\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#setting-up-hyperparamters\",\n    \"aria-label\": \"setting up hyperparamters permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Setting up Hyperparamters\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Batch size: 32\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Learning rate : 2e-5\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Epochs : 2\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Epsilon : 1e-8 (to avoid small number division by zero)\")), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"h2\", {\n    \"id\": \"let-the-training-begin\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#let-the-training-begin\",\n    \"aria-label\": \"let the training begin permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Let the Training Begin!\"), mdx(\"p\", null, \"For each pass, we have a training and a validation phase. Let us understand the core of each of these phases.\"), mdx(\"h4\", {\n    \"id\": \"training\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h4\"\n  }, {\n    \"href\": \"#training\",\n    \"aria-label\": \"training permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Training:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Unpack the data and labels from the iterator\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Loading the data on the GPU (to.device)\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Clearing the gradients from previous pass\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Forward pass through the model\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Backward pass\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Updating model parameters (optimizer.step())\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Save variables for each step/epoch monitoring\")), mdx(\"h4\", {\n    \"id\": \"validation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h4\"\n  }, {\n    \"href\": \"#validation\",\n    \"aria-label\": \"validation permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Validation:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Unpack the data and labels from the iterator\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Loading the data on the GPU (to.device)\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Forward pass through the model\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Compute Loss on the validation data\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Save variables for each step/epoch monitoring\")), mdx(\"p\", null, \"Now that the training flow is understood, let us look at the code.\"), mdx(CopyBlock, {\n    language: \"python\",\n    text: \"for _ in tnrange(1,epochs+1,desc='Epoch'):\\n  print(\\\"<\\\" + \\\"=\\\"*22 + F\\\" Epoch {_} \\\"+ \\\"=\\\"*22 + \\\">\\\")\\n  # Calculate total loss for this epoch\\n  batch_loss = 0\\n  for step, batch in enumerate(train_dataloader):\\n    # Set our model to training mode (as opposed to evaluation mode)\\n    model.train()\\n    \\n    # Add batch to GPU\\n    batch = tuple(t.to(device) for t in batch)\\n    \\n    # Unpack the inputs from our dataloader\\n    b_input_ids, b_input_mask, b_labels = batch\\n    \\n    # Forward pass\\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\\n    loss = outputs[0]\\n    \\n    # Backward pass\\n    loss.backward()\\n    \\n    # Clip the norm of the gradients to 1.0\\n    # Gradient clipping is not in AdamW anymore\\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n    \\n    # Update parameters and take a step using the computed gradient\\n    optimizer.step()\\n    \\n    # Update learning rate schedule\\n    scheduler.step()\\n    \\n    # Clear the previous accumulated gradients\\n    optimizer.zero_grad()\\n    \\n    # Update tracking variables\\n    batch_loss += loss.item()           \\n  # Calculate the average loss over the training data.\\n  avg_train_loss = batch_loss / len(train_dataloader)\\n  #store the current learning rate\\n  \\n  for param_group in optimizer.param_groups:\\n    print(\\\"\\n\\tCurrent Learning rate: \\\",param_group['lr'])\\n    learning_rate.append(param_group['lr'])\\n    train_loss_set.append(avg_train_loss)\\n  print(F'Average Training loss: {avg_train_loss}')\\n  \\n  # Validation\\n  \\n  # Put model in evaluation mode to evaluate loss on the validation set\\n  model.eval()\\n  \\n  # Tracking variables \\n  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\\n  # Evaluate data for one epoch\\n  for batch in validation_dataloader:\\n    # Add batch to GPU\\n    batch = tuple(t.to(device) for t in batch)\\n    # Unpack the inputs from our dataloader\\n    b_input_ids, b_input_mask, b_labels = batch\\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\\n    with torch.no_grad():\\n      # Forward pass, calculate logit predictions\\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\\n    \\n    # Move logits and labels to CPU\\n    logits = logits[0].to('cpu').numpy()\\n    label_ids = b_labels.to('cpu').numpy()\\n    \\n    pred_flat = np.argmax(logits, axis=1).flatten()\\n    \\n    labels_flat = label_ids.flatten()\\n    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\\n    \\n    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\\n    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\\n    \\n    eval_accuracy += tmp_eval_accuracy\\n    eval_mcc_accuracy += tmp_eval_mcc_accuracy\\n    \\n    nb_eval_steps += 1\\n  \\n  print(F'Validation Accuracy: {eval_accuracy/nb_eval_steps}')\\n  print(F'Validation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\",\n    showLineNumbers: false,\n    theme: dracula,\n    wrapLines: true,\n    codeBlock: true,\n    mdxType: \"CopyBlock\"\n  }), mdx(\"p\", null, \"Currently we have performed no hyperparameter tuning. Adjustments can be made in altering the learning rate, epochs, batch size, ADAM properties . I leave them for you to test and find the best set. Do let me know if you find some crazy differences. Note, how we did not train on the entire training dataset and kept  aside a portion of it as our validation set for legibility of code. Another thing that impacts the overall performance is the random seed. Maybe, play with that too and notice the results. Finally, you can find the entire code on Github.\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"Let's Play with Emotions! In today’s tutorial, we will play with emotions, quite literally! With the growing trends in machine learning and…"}},"pageContext":{"slug":"/2021-03-25-let's-play-with-emotions/"}},"staticQueryHashes":["1623602484","2744905544","3262950449"]}